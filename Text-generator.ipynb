{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package brown to /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package names to /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package timit to /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package udhr to /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package punkt to /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/anand/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read  66  files...\n",
      "corpus length: 1915949\n"
     ]
    }
   ],
   "source": [
    "corpora_dir = \"/home/anand/nltk_data/corpora/state_union\"\n",
    "\n",
    "# Read all file paths in corpora directory\n",
    "file_list = []\n",
    "for root, _ , files in os.walk(corpora_dir):  \n",
    "    for filename in files:\n",
    "        file_list.append(os.path.join(root, filename))\n",
    "        \n",
    "print(\"Read \", len(file_list), \" files...\" )\n",
    "\n",
    "# Extract text from all documents\n",
    "docs = []\n",
    "\n",
    "for files in file_list:\n",
    "    with open(files, 'r') as fin:\n",
    "        try:\n",
    "            str_form = fin.read().lower().replace('\\n', '')\n",
    "            docs.append(str_form)\n",
    "        except UnicodeDecodeError: \n",
    "            # Some sentences have wierd characters. Ignore them for now\n",
    "            pass\n",
    "# Combine them all into a string of text\n",
    "text = ' '.join(docs)\n",
    "\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Unique Characters: 57\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('Total Number of Unique Characters:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars)) # Character to index\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars)) # Index to Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 638637\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Recommended to run this script on GPU, as recurrent\n",
    "networks are quite computationally intensive.\n",
    "If you try this script on new data, make sure your corpus\n",
    "has at least ~100k characters. ~1M is better.\n",
    "\"\"\"\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40 # Number of characters considered\n",
    "step = 3 # Stide of our window\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "# Rading the text in terms of sequence of characters\n",
    "# Extract only 'maxlen' characters every time\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    # The character just after the sequence is the label\n",
    "    next_chars.append(text[i + maxlen]) \n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "# Initializing Tensor (training data)\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool) \n",
    "# Initializing Output that holds next character (label)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool) \n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        # Populate Tensor Input\n",
    "        x[i, t, char_indices[char]] = 1 \n",
    "    # Populate y with the character just after the sequence\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"Perform Temperature Sampling\"\"\"\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature \n",
    "    exp_preds = np.exp(preds)\n",
    "    # Softmax of predictions\n",
    "    preds = exp_preds / np.sum(exp_preds) \n",
    "    # Sample a single characters, with probabilities defined in `preds`\n",
    "    probas = np.random.multinomial(1, preds, 1) \n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    \"\"\"Function invoked at end of each epoch. Prints generated text\"\"\"\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- Diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            # Generate next character\n",
    "            next_index = sample(preds, diversity) \n",
    "            next_char = indices_char[next_index]\n",
    "            \n",
    "            # Append character to generated sequence\n",
    "            generated += next_char \n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "    \n",
    "    # Save model weights into file\n",
    "    model.save_weights('saved_weights.hdf5', overwrite=True)\n",
    "        \n",
    "\n",
    "# After every single epoch, we are going to call the function on_epoch_end\n",
    "# to generate some text.\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "checkpointer = ModelCheckpoint(filepath='/tmp/weights.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Epoch 1/30\n",
      "4983/4990 [============================>.] - ETA: 0s - loss: 2.1337\n",
      "----- Generating text after Epoch: 0\n",
      "----- Diversity: 0.2\n",
      "----- Generating with seed: \"vidual standards. excellence is what mak\"\n",
      "vidual standards. excellence is what make to the promed that the recore and the prople and the prosed to the progres the strent of the propors that a stand to state the prople the stive the prople the rest the progres and the sear and the sout the progres in the contress to propes that the state that the can the conerity that the sear the prople the rest and the propore the sere the prosed the sere the proment in the proved that the pro\n",
      "----- Diversity: 0.5\n",
      "----- Generating with seed: \"vidual standards. excellence is what mak\"\n",
      "vidual standards. excellence is what make the pare that is the cention are and the sere the cenure and our most and it a continity and the preess for be the lowl the work in that and and chill shich furt the rest and be bull the conment to the prome and we mise of the progres of the must can a reace of the har the enition and the bust far and and the promest to are the peaple of the enderts will state to mast the storl the bect and it i\n",
      "----- Diversity: 1.0\n",
      "----- Generating with seed: \"vidual standards. excellence is what mak\"\n",
      "vidual standards. excellence is what make if the wewlli greend ecsineces -in the peos ination-bara.fremer we end as lage risp de oubled mijait to provid the polnod moblicaly ussisily this ny suctar awerrenthi, the a depent cingrle, sat standation ar and conerimactoraminity prousg dustcat coming beldentstap ist promatious afdanthing allentery it and one (ampyobenting love and tiate love bate espantinition that to hene prope pricapolion a\n",
      "----- Diversity: 1.2\n",
      "----- Generating with seed: \"vidual standards. excellence is what mak\"\n",
      "vidual standards. excellence is what mak poonlay actaqurett our our, this  tourt ry medial parifations workend ficteance oa haved dentwith irsbest cotmarted aw  ou wishl's greaschtlif have with the ereflai horod if offurense an the oirs foverty. the post not for.menchoving eteadory ial thes, we't motee remsienet. it a.d elwole wark. at what sudibl ereslanis aye, i slard the nedivlicas, efous coltive furane )hie .x warve aversther with.'\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "4990/4990 [==============================] - 81s 16ms/step - loss: 2.1334\n",
      "Epoch 2/30\n",
      "4986/4990 [============================>.] - ETA: 0s - loss: 1.6846\n",
      "----- Generating text after Epoch: 1\n",
      "----- Diversity: 0.2\n",
      "----- Generating with seed: \"rway which will assure we can deter war \"\n",
      "rway which will assure we can deter war and the congress of the congress of the congress of the defending defense of our comminity to the resoress of the forces and the congress of the congress to sere a strength in the congress of the for and strength and the congress of the program of the world of the congress and to are the congress to service and the nation of the soulity to be and the congress to and the congress and the nation of \n",
      "----- Diversity: 0.5\n",
      "----- Generating with seed: \"rway which will assure we can deter war \"\n",
      "rway which will assure we can deter war a never community, the recold and restrong the created saince and the raties and have on the warten of congress of encherse the trome and freedom and to we make moderness of lecas and conserte to mast persority from propose in a surces of our confrotion of comminity for the world resorme beson not cores, and make of the for all but acting their contricul the nations of the mast resorvical progress\n",
      "----- Diversity: 1.0\n",
      "----- Generating with seed: \"rway which will assure we can deter war \"\n",
      "rway which will assure we can deter war of when hough for deees croplic peacting ferciper it conege. now they the sellor, bother wastical byso. must towond we can intail this younting of sheeld, arel to do stren-lots progres, and of the not betymentrivush in by not aberist no and now behave to musterser-terpor of defenve new the deefore and edcoural reforement freaser bright and entirat beatiou life of growhnce.wo the nation of were, ar\n",
      "----- Diversity: 1.2\n",
      "----- Generating with seed: \"rway which will assure we can deter war \"\n",
      "rway which will assure we can deter war it rangichionces, tring uners actoons resiniation that auspeed toirg ore yeans ledes of when a hodews poweratial dielda. thete prolated straha dection spefice st mubler, reduceder, in licestive emblesarm5 diffor--completeres. theregution.spengly degidats producthe blesso feveropen firefation nationt. activened flleselns uning repastsh to leves infiral gold'sm. to ast hast conmmotiines arovicl dast\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "4990/4990 [==============================] - 79s 16ms/step - loss: 1.6845\n",
      "Epoch 3/30\n",
      "4983/4990 [============================>.] - ETA: 0s - loss: 1.5144\n",
      "----- Generating text after Epoch: 2\n",
      "----- Diversity: 0.2\n",
      "----- Generating with seed: \"ead.  values are important, so we've tri\"\n",
      "ead.  values are important, so we've triee and the congress to the people of the people of the propress of the world and the congress and the proposal to the congress and the rest the congress to the first the world and the congress and the congress of the security of the first the first to the congress to the congress and states of the finds and the fidence and in the world that we will be a strength of the congress of the long the con\n",
      "----- Diversity: 0.5\n",
      "----- Generating with seed: \"ead.  values are important, so we've tri\"\n",
      "ead.  values are important, so we've triee forture the world and do not and with the infression and production.to the partic propriation can be the people so means and for the recommended to the bening and may and the propress to the nation will be more the parting in the tax and the last is i pall that the new in the from the congress and the free the people of the finds with i will be the propation and be an and the the from of sen to\n",
      "----- Diversity: 1.0\n",
      "----- Generating with seed: \"ead.  values are important, so we've tri\"\n",
      "ead.  values are important, so we've trick to the edication have for agricumars of do spending. this need inca now rest ments, 1pjacts af the low, therecoon of monture of the congress so fith eecopolic hard the figst of the world in pirents.--s time of the peicenty and helper efforts which the finalenity for a realletimuagely by come. and you will strengthe,. se wifling and slestion togext in medit and incomes at dedays. throughlan and \n",
      "----- Diversity: 1.2\n",
      "----- Generating with seed: \"ead.  values are important, so we've tri\"\n",
      "ead.  values are important, so we've trien to fact.monigred and this hi drigndafityory, the loniteate. this has heil. gecm on the spucit will remornite, against theigh regurme. we a1 holedver -o to and seturation over insial to be conluctly hart in thas 120 billion dollives toot. but hersodic consurtop that7s--prosist govestpupation through as vect ubaid so alak, to me te revays of to cupporumit, for demb, the day.- who leates for intod\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "4990/4990 [==============================] - 75s 15ms/step - loss: 1.5143\n",
      "Epoch 4/30\n",
      "4985/4990 [============================>.] - ETA: 0s - loss: 1.4233\n",
      "----- Generating text after Epoch: 3\n",
      "----- Diversity: 0.2\n",
      "----- Generating with seed: \"e one. i hope you will support that, as \"\n",
      "e one. i hope you will support that, as the congress and the congress of the people and interest of the congress to the congress to be an and the congress of the congress to the congress to the congress to the congress of the congress to the congress to be an and interest of the defense of the congress to the congress and the first to the congress to the congress to do the defense the congress and the congress to be a strength and to th\n",
      "----- Diversity: 0.5\n",
      "----- Generating with seed: \"e one. i hope you will support that, as \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e one. i hope you will support that, as the world are not budget the war in our country that the world and service will strake that is the security hos i can go all our new action, and the common for which control of vition of the interest and assist and to the people for and the state of the world in the control american programs and have see t"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "# Size of vector in the hidden layer.\n",
    "hidden_size = 128 \n",
    "# Initialize Sequential Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_size, input_shape=(maxlen, len(chars))))\n",
    "# Add the output layer that is a softmax of the number of characters\n",
    "model.add(Dense(len(chars), activation='softmax')) \n",
    "# Optimization through RMSprop\n",
    "optimizer_new = RMSprop() \n",
    "# Consider cross Entropy loss. Why? MLE of P(D | theta)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer_new) \n",
    "\n",
    "# Train this for 30 epochs. Size of output from LSTM i.e. hidden layer vector shape=128\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=30,\n",
    "          callbacks=[print_callback, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
